name: "qwen_model"
backend: "vllm"

# Standard LLM configuration for Triton vLLM backend
parameters: {
  key: "model_path"
  value: { string_value: "/models/deployed_model" }
}

# Add any vLLM specific arguments here
parameters: {
  key: "vllm_model_additional_args"
  value: { string_value: "--gpu-memory-utilization 0.9 --enforce-eager" }
}

# Define the input/output for Triton requests
input [
  {
    name: "text_input"
    data_type: TYPE_STRING
    dims: [ 1 ]
  }
]
output [
  {
    name: "text_output"
    data_type: TYPE_STRING
    dims: [ -1 ]
  }
]
